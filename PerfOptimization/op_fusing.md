å¯¹äºæ·±åº¦å­¦ä¹ æ¥è¯´ï¼Œå…¶æ¨¡å‹è®¡ç®—é‡å¾€å¾€å¾ˆå¤§ï¼ˆè®­ç»ƒå°¤ç”šï¼‰ï¼Œä½†åœ¨è®­ç»ƒæ—¶ç»å¸¸çœ‹åˆ°GPUåˆ©ç”¨ç‡æ‰“ä¸æ»¡çš„æƒ…å†µï¼Œè¿™è¯´æ˜ç“¶é¢ˆåœ¨GPUç®—åŠ›ä»¥å¤–çš„å…¶ä»–åœ°æ–¹ï¼Œå…¶ä¸­ä¸€ä¸ªæœ€é‡è¦çš„å½±å“å› ç´ ä¾¿æ˜¯æ˜¾å­˜å¸¦å®½

ä¸‹è¡¨åˆ—ä¸¾äº†å‡ ä¸ªå¸¸ç”¨æ˜¾å¡çš„å¸¸è§„å‚æ•°

| Model       | Memory (GB) | Memory Bandwidth (GB/sec) | FP32 TFLOPS | FP16 TFLOPS |
| ----------- | ----------- | ------------------------- | ----------- | ----------- |
| A100        | 80          | 2039                      | 19.5        | 312         |
| V100        | 16          | 900                       | 15.7        | 125         |
| A6000       | 48          | 768                       | 38          | 150         |
| RTX 3090 TI | 24          | 1008                      | 40          | 160         |

ä»¥RTX 3090TIä¸ºä¾‹ï¼Œå…¶æ˜¾å­˜å¸¦å®½ä¸º1008GB/sï¼Œå‡å¦‚æˆ‘ä»¬åœ¨è¿™ä¸Šé¢åšelementwiseçš„float32åŠ æ³•è¿ç®—ï¼Œè€ƒè™‘åˆ°åŠ æ³•è¿ç®—éœ€è¦å…ˆæŠŠtensorä»æ˜¾å­˜è¯»å‡ºï¼Œè®¡ç®—å‡ºç»“æœåå†å†™å›æ˜¾å­˜ï¼Œé‚£ä¹ˆå…¶æ¯ç§’æœ€å¤šåªèƒ½åš`1008GB/4/2=126GB/s`æ¬¡åŠ æ³•è¿ç®—ï¼Œè¿œè¿œè¾¾ä¸åˆ°å…¶ç†è®ºä¸Šé™å€¼40TFLOPS

```python
# Xï¼ŒYå‡ä¸ºfloat32å‘é‡
Y = X + 2
```
å½“ç„¶ï¼ŒçœŸå®æƒ…å†µä¸ä¼šè¿™ä¹ˆæç«¯ï¼Œè™½ç„¶ç½‘ç»œæ¶æ„ï¼ˆCNNï¼ŒRNNï¼ŒTransformersç­‰ï¼‰æœ‰å¾ˆå¤šç§ï¼Œä½†ç®—å­ç§ç±»æœ‰é™ï¼Œå…¶ä¸­çŸ©é˜µä¹˜æ³•æ˜¯æ¯”è¾ƒè®¡ç®—å¯†é›†å‹çš„ï¼Œ**elementwise op**æ˜¯æ¯”è¾ƒåå†…å­˜copy/writeçš„ï¼Œç”±æ­¤ä¼šæ‹‰ä½æ¨¡å‹çš„æ•´ä½“TFLOPSï¼Œæˆ‘ä»¬ä¼˜åŒ–æ¨¡å‹æ€§èƒ½çš„ä¸€ä¸ªå¾ˆé‡è¦çš„æ–¹å‘ä¾¿æ˜¯å‡å°‘elementwise opçš„ä¸ªæ•°å·²è¾¾åˆ°å‡å°‘æ˜¾å­˜æ•°æ®read/writeçš„ç›®çš„, æ­¤æ—¶ç®—å­èåˆå°±æ˜¾å¾—å¾ˆé‡è¦äº†ï¼Œå¦‚ä¸‹ä»‹ç»æ¥è‡ªChatGPTç”Ÿæˆçš„å›ç­”ï¼ˆå†™å¾—æ¯”æˆ‘å¥½å¤šäº†ï¼Œç›´æ¥è´´åœ¨è¿™é‡Œï¼‰ï¼š

**Operation fusion** (also called **op fusion**) is a performance optimization technique widely used in deep learning frameworks and compilers. It **combines multiple operations into a single kernel** or computational step to improve efficiency.

---

## ğŸš€ What Is Operation Fusion?

Instead of launching separate kernels (on CPU/GPU) for each operation, **fusing them** means:

* **Reducing kernel launch overhead**
* **Avoiding intermediate memory writes/reads**
* **Enabling better parallelization and caching**

> âœ… Example:

```python
# Unfused operations
y = x * 2         # kernel 1
z = relu(y)       # kernel 2
```

* Requires **two CUDA kernel launches**
* Intermediate result `y` is **written to memory** after kernel 1, and **read again** in kernel 2

```python
# Fused version
z = fused_mul_relu(x)
```

* **One kernel**
* No intermediate memory write for `y`, just compute and proceed directly to ReLU

ğŸ” **Same total computation** (still multiply + ReLU), but **much faster** due to lower memory traffic and launch overhead.

---

## ğŸ”§ Types of Operation Fusion

There are several common categories of op fusion used in practice:

### 1. **Elementwise Operation Fusion (Pointwise Fusion)**

* Fuse sequences of elementwise ops like `add`, `mul`, `relu`, `sigmoid`.
* Very common because elementwise ops are easy to chain.
* Example: `y = relu(x * scale + bias)`

### 2. **Activation + Linear Fusion**

* Fuse activation functions directly after linear layers.
* Example: `y = relu(matmul(x, W))` becomes a fused matmul + relu op.

### 3. **Convolution + Bias + Activation**

* Extremely common in CNNs.
* Fuse: `conv2d â†’ bias add â†’ activation (e.g., ReLU)`
* Improves GPU throughput and avoids intermediate buffer storage.

### 4. **Normalization + Activation Fusion**

* Example: `batch_norm + relu`, or `layer_norm + gelu`
* Used in BERT/GPT-style Transformer blocks.

### 5. **Transpose + MatMul Fusion**

* In attention mechanisms, it's common to fuse `transpose â†’ matmul` patterns.
* Helps eliminate extra memory reads/writes.

### 6. **Reshape/View + Elementwise Fusion**

* Sometimes reshapes are just metadata operations; fusing them avoids unnecessary compute.

---

## ğŸ§  Fusion in Practice (by Framework)

| Framework                           | Fusion Approach                                                   |
| ----------------------------------- | ----------------------------------------------------------------- |
| **TensorFlow XLA**                  | Uses graph-level compiler to fuse compatible ops into HLO kernels |
| **PyTorch (TorchScript + nvFuser)** | Supports runtime or JIT fusion for CUDA kernels                   |
| **ONNX Runtime**                    | Fuses subgraphs during optimization (e.g., conv+bn+relu)          |
| **TVM**                             | Uses scheduling and pattern matching to generate fused kernels    |
| **TensorRT**                        | Performs aggressive layer fusion during model conversion          |

---

## âš™ï¸ Why Fusion Matters


| Optimization               | How It Helps                                                                          |
| -------------------------- | ------------------------------------------------------------------------------------- |
| **Memory bandwidth**       | Fewer reads/writes of intermediate results to global GPU memory                       |
| **Kernel launch overhead** | Fewer individual kernels â†’ fewer scheduling/launch delays                             |
| **Better data locality**   | More chances to keep data in **registers/shared memory**, avoiding slow global memory |
| **Increased parallelism**  | Enables more efficient GPU thread scheduling and usage of warp execution              |

Note that op fusion does not reduce the amount of computation in terms of math operations (like adds/muls), it reduces the computation overhead.

---

## ğŸ§ª Limitations or Challenges

* **Not all ops are fusible** (e.g., if data dependencies are complex).
* **Numerical accuracy** may slightly vary after fusion due to reordering.
* **Fusing too much** can make debugging/tracing harder.
* Some ops need **shape alignment** or **broadcast constraints** to fuse.

---

## âœ… Summary Table

| Fusion Type              | Example                   |
| ------------------------ | ------------------------- |
| Elementwise              | `relu(x + bias)`          |
| Linear + Activation      | `relu(matmul(x, W))`      |
| Conv + Bias + Activation | `relu(conv2d(x, W) + b)`  |
| Norm + Activation        | `gelu(layer_norm(x))`     |
| Transpose + Matmul       | `matmul(transpose(x), y)` |

---


## å‚è€ƒé“¾æ¥
- æ¥è‡ªæ²ç¥çš„[transformers-benchmarks](https://github.com/mli/transformers-benchmarks/tree/main)
- æ²ç¥çš„[è§†é¢‘è®²è§£](https://www.bilibili.com/video/BV1LT411F77M?spm_id_from=333.788.videopod.sections&vd_source=4e29e34ec0b6dc6503fee23f1aa2bfee)ï¼Œå…¶ä¸­è¯¦ç»†ä»‹ç»äº†ä¸Šé¢benchmarkè„šæœ¬çš„ç”¨æ³•ï¼Œå’Œä¸åŒtransformerç»“æ„çš„æµ‹è¯•ç»“è®º
